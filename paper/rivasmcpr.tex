\documentclass[a4paper]{article}
\usepackage[letterpaper, margin=1in]{geometry} % page format
\usepackage{listings} % this package is for including code
\usepackage{amsmath}  % this package is for math and matrices
\usepackage{amsfonts} % this package is for math fonts
\usepackage{hyperref} % for urls
\usepackage{dirtree}  % for directory trees
\usepackage{graphicx} % for figures
\usepackage{tikz}     % for special diagrams   -----------------
\usetikzlibrary{positioning,calc}
\tikzset{
  redondo/.style={
    draw=black,
    line width=1pt,
    rounded corners=3pt,
    minimum height=1.2em,
    text width=#1
  },
  punto/.style={
    circle,
    inner sep=0pt,
    text width=2.5mm,
    fill=gray!50!black
  },
  tresp/.pic={
    \node[punto] at (0.25,0) {};
    \node[punto] at (0.5,0) {};
    \node[punto] at (0.75,0) {};
  },
  trespg/.pic={
    \node[punto,fill=gray!90!black] at (0.25,0) {};
    \node[punto,fill=gray!90!black] at (0.5,0) {};
    \node[punto,fill=gray!90!black] at (0.75,0) {};
  },
  dosp/.pic={
    \node[punto] at (0.25,0) {};
    \node[punto] at (0.5,0) {};
  },
  unop/.pic={
    \node[punto] at (0.5,0) {};
  },
  cuadra/.style={
    fill=white,
    line width=0.1pt,
    minimum size=0.1pt
  },
  arr/.style={
    line width=1pt,
    draw=gray,
    ->,
    >=latex
  }  
}
%------------------------------- end of special diagrams
\usepackage{pgfplots} % for bar plots -------------------------
\pgfplotsset{
  % use this `compat' level or higher to use the advanced positioning
  % features of `nodes near coords' in stacked bar plots
  compat=1.9,
}


\title{A deep learning approach to sign language recognition using stacked
sparse autoencoders}
\author{Pablo Rivas}
\date{Jan/24/17}

\begin{document}
\lstset{language=Python}

\maketitle

\begin{abstract}
In this paper we address the problem of hand gestures recognition in the
American Sign Language (ASL) using deep learning. We propose a five layer model
using unsipervised encoder-decoder layers and a softmax layer in the output. We
use a dataset of segmented images captured with a depth-sensor camera for
different subjects. The average accuracy obtained was of 98.9\% and as high as
99.4\% on unseen data.
\end{abstract}

\section{Introduction}

Language is an essential part of being human as it enables us to communicate
with others. Some people have to develop additional means of communication
for different reasons. A popular alternative language is through signs. This
type of language is based on symbols or figures produced using the hands. 

Sign language recognition is a task that has been studied in the last few years
\cite{starner1998real}. We believe this is an important problem to address
because it directly affects the quality of life of people who need to
communicate using sign language. The task is to develop algorithms that can
recognize different signs in an alphabet and that can do so in an efficient
manner. Ideally, the methodologies should be fast to the point of enabling the 
near real-time processing of signs \cite{starner1997real}.

In this research we deal with the recognition of signs over the american sign
language (ASL). The proposed approach uses depth images of subjects making
different signs, building upon the work of B. Kang, et.al. in
\cite{kang2015real}. 

Typical approaches involve the using hidden Markov models
\cite{starner1997real}, and a combination of them with other discriminative
functions for feature extraction in multi-stage architectures
\cite{lichtenauer2008sign}. Other major alternatives included the exploration
of neural strategies combined with fuzzy systems \cite{al2001recognition}. For
a more detaile review of alternatives for generic hand gesture recognition we
can turn to the work in \cite{murthy2009review}.

However, with the dramatic attention that deep learning has gained recently 
in the machine learning and the immage processing for pattern recognition
communities, we are motivated to similarly explore this new algoritmic
alternative to see if it offers better solutions to open problems. Most
recently, the authors in \cite{kang2015real} have explored a deep learning
approach based on convolutional neural networks (CNNs) achieving outstanding
results. The research presented here also explores a novel deep learning
approach but based in autoencoders and neural networks. We will show that this
alternative approach achieves great performance and efficiency.

The rest of the paper is organized as follows. 


\section{Background and Related Work}

Recently, J. Nagi et.al. \cite{nagi2011max} studied a deep neural network
approach for recognizing six different hand gestures given to a robot. Using
CNNs the authors achieve a 96\% accuracy rate. Then, M. Van den Bergh et.al.
\cite{van2011combining} explored the idea of using depth-sensor imagery to
establish an algorithm for hand segmentation. The authors ultimately used this
to recognize six different hand gestures in 3D. They achieved a 99.54\%
recognition rate. Some of the key commonents of the overall architecture were
the usage of wavelets for image filtering and neural networks.

Moreover, B. Kang et.al. \cite{kang2015real} took advantage of the recent
success of CNNs for pattern recognition over images. They propose a system that
makes use of depth-sensor images to study further the problem of hand gesture
recognition. When recognizing signs within the same subject they averaged a 99.99\%
accuracy rate over five subjects. However, when recognizing signs across
different subjects, their accuracy rate droped to 83.58\%. 

Our research aims to further study the problem posed in \cite{kang2015real} of
increasing accuracy rates over different subjects. In the next section we
discuss the dataset used and propose a deep learning approach using
autoencoders and a neural network. 


\section{Methodology}

\subsection{Data}

This research focuses in the ASL as a case study \cite{baker1980american}. 
The ASL has 26 hand gestures 
corresponding to the letters in the English alphabet; it also contains 9 hand
gestures for every single number. Figure \ref{fig:asl} 
depicts the gestures pertaining to our study.

\begin{figure}
\centering
\includegraphics[width=12.2cm]{img/signs.eps}
\caption{The American Sign Language (ASL) displaying 26 signs corresponding to the
English alphabet (A to Z) and 9 signs for numbers (1 to 9).}
\label{fig:asl}
\end{figure}

We use the data provided in \cite{kang2015real} that was released in 2015. This
dataset consists of images acquired with a three-dimensional depth-sensor camera. 
There is a total of $31,000$ images available with a resolution of $256 \times
256$. The images correspond to already segmented dept-images that contain the
area of the hand. The authors use a rather simple algorithm to make the
segmentation based on the fact that in all depth images the hand is the closest
object with respect to the camera. 

Not all the 35 signs (26 letters and 9 numbers) 
were considered in this study. Some signs
were exluded and others considered jointly as follows.

The signs corresponding to J and Z clearly require a sequence of images rather
than a single instance, as can be seen in Figure \ref{fig:asl}. This goes
beyond the scope of this project; although the detection of signs from
live-video sequences is part of a bigger project, we will not address it in this
paper. For that reason these two signs were exluded.

The signs corresponding to V and 2, by observation, are nearly identical and
their distinction requires contextual information. Similarly, the signs for
letter W and number 6 require being interpreted in context. 
For this reason, these signs were considered as the same sign, leaving a total
of 31 signs.

There is a total of five different 
subjects and each subject produced 200 images of each
sign. This represents a total 1,000 images per sign and 31,000 images overall.
The images collected capture each subject making a hand gesture moving across
the horizontal axis and varying inclination of the hand gesture, as depicted in
Figure \ref{fig:sampN1}.

\begin{figure}
\centering
\includegraphics[width=12.2cm]{img/sampleNum1.eps}
\caption{Examples of ASL hand gestures corresponding to the number one and 
number two with variations in
the horizontal axis and its inclination with respect to a depth-sensor camera.}
\label{fig:sampN1}
\end{figure}


\subsection{Sparse Autoencoders}

A sparse autoencoder is usually categorized as a neural network with
unsupervised learning \cite{le2013building}. In a general sense an autoencoder
is trained to output an approximation of the input provided a deep architecture
of layered neurons that encode and decode based on the input stimuli. 
In our research we use a sparse autoencoder that specifically
minimizes a modified loss function based on the mean squared error. 

Let $\mathbf{x} \in \mathbb{R}^d$ be a $d$-dimensional input vector. Then the
loss function of the common sparse autoencoder is defined as follows:
\begin{align}
L=\frac{1}{N} \left\| \mathbf{x}_n - \mathbf{\hat{x}}_n  \right\|^2_2
+ \theta_w \frac{1}{2} \sum^L_{l=1} \left\| \mathbf{w}^l \right\|^2_2 +
\theta_s \sum^{M}_{m=1} KL \left( \theta_\alpha \left\| \bar{\alpha}_m \right. \right)
\label{eq:loss}
\end{align}
where $N$ is the total number of training samples; 
$\mathbf{\hat{x}}_n$ is the learned (or encoded) approximation of the
$n$-th input vector $\mathbf{x}_n$;
$\theta_w$ controls the sparsenes of the weights of the network, $\mathbf{w} \in
\mathbb{R}^d$;
$L$ denotes the number of layers in the deep network;
$\theta_s$ regulates the sparsity of the activation functions' output,
$\alpha$, in every neuron in the network;
$M$ is the total number of neurons in the deep network;
and $KL(\cdot)$ is the Kullback-Leibler divergence function 
\cite{joyce2011kullback} used to measure how
much the observed average activation of the $m$-th neuron, $\bar{\alpha}_m$,
actually deviates from the desired average output, $\theta_\alpha$.

The Kullback-Leibler divergence function can be defined as follows
\cite{olshausen1997sparse}: 
\begin{align}
\sum^M_{m=1} KL \left( \theta_\alpha \left\| \bar{\alpha}_m \right. \right) = 
\sum^M_{m=1} \theta_\alpha \log \left( \frac{\theta_\alpha}{\bar{\alpha}_m} \right)
+ (1 - \theta_\alpha) \log \left( \frac{1-\theta_\alpha}{1-\bar{\alpha}_m} \right)
\label{eq:divergence}
\end{align}
where the average output of the $m$-th neuron, $\bar{\alpha}_m$, at the $l$-th
layer is given by
\begin{align}
\bar{\alpha}_m = \frac{1}{N} \sum^N_{n=1} \psi \left( \mathbf{w}^{(l)T}_m
\mathbf{x}_n + b^{(l)}_m \right)
\end{align}
where $\psi(\cdot)$ is the neuron's activation function, and
$b^{(l)}_m$ is the bias term for the $m$-th neuron at the $l$-th layer. 
In this research we specifically use logistic (sigmoid) activation functions,
$\psi(z) = 1/(1-e^{-z})$, for any given value of $z$. 

By observing the mathematical form of (\ref{eq:loss}), it is feasible to apply
an accelerated form of guided learning known as scaled conjugate gradient (SCG)
descent \cite{moller1993scaled}. SCG has proven to be a reliable and efficient
form of minimizing loss functions such as the one for the sparse autoencoder,
overcoming the shorcomings of a traditional conjugate gradient and
a back-propagation with gradient descents \cite{le2013building}. 


\subsection{Deep Learning Architecture}

Two stacked autoencoders are combined with a feedforward neural network in a
five-layer architecture, as shown in Figure \ref{fig:TrainingArchitecture}. 
\begin{figure}
\centering
\begin{tikzpicture}[node distance=1.5cm and 1.25cm]
\node[redondo=2cm,label={120:31 neural units}]
  (upper)
  {};
\pic at (upper.west) {dosp};
\pic at ([xshift=-0.75cm]upper.east) {unop};
\node[] 
  at ([xshift=4pt,yshift=-1pt]$ (upper.center) $ ) {$\ldots$};


\node[redondo=6.4cm,draw=gray,below=of upper,label={140:100 neural units}]
  (middleDec)
  {};
\pic at (middleDec.west) {trespg};
\pic at ([xshift=-1.85cm]middleDec) {trespg};
\pic at ([xshift=-2.58cm]middleDec) {trespg};
\pic at ([xshift=-1cm]middleDec.east) {trespg};
\node[] at ([xshift=4pt,yshift=-1pt]middleDec) {$\ldots$};



\node[redondo=3.3cm,below=of middleDec,label={140:50 neural units}]
  (middle)
  {};
\pic at (middle.west) {tresp};
\pic at ([xshift=-1.04cm]middle) {dosp};
\pic at ([xshift=-0.75cm]middle.east) {dosp};
\node[] at ([xshift=4pt,yshift=-1pt]middle) {$\ldots$};


\node[redondo=12cm,draw=gray,below=of middle,label={140:65536 neural units}]
  (lowerMiddleDec)
  {};
\pic at (lowerMiddleDec.west) {trespg};
\pic at ([xshift=-5.38cm]lowerMiddleDec) {trespg};
\pic at ([xshift=-4.65cm]lowerMiddleDec) {trespg};
\pic at ([xshift=-3.92cm]lowerMiddleDec) {trespg};
\pic at ([xshift=-3.19cm]lowerMiddleDec) {trespg};
\pic at ([xshift=-2.46cm]lowerMiddleDec) {trespg};
\pic at ([xshift=-1.73cm]lowerMiddleDec) {trespg};
\pic at ([xshift=-1.75cm]lowerMiddleDec.east) {trespg};
\pic at ([xshift=-1.00cm]lowerMiddleDec.east) {trespg};
\node[] at ([xshift=4pt,yshift=-1pt]lowerMiddleDec) {$\ldots$};


\node[redondo=6.4cm,below=of lowerMiddleDec,label={140:100 neural units}]
  (lowermiddle)
  {};
\pic at (lowermiddle.west) {tresp};
\pic at ([xshift=-1.85cm]lowermiddle) {tresp};
\pic at ([xshift=-2.58cm]lowermiddle) {tresp};
\pic at ([xshift=-1cm]lowermiddle.east) {tresp};
\node[cuadra,below=of lowermiddle,label={right:{training images}}] (cuadramiddle)
{\includegraphics[width=.075\textwidth]{img/S01num1.eps} 
\includegraphics[width=.075\textwidth]{img/S01num2.eps}
$\cdots$ \includegraphics[width=.075\textwidth]{img/S01num31.eps}};

\node[] at ([xshift=4pt,yshift=-1pt]lowermiddle) {$\ldots$};

\draw[arr,draw=black] (cuadramiddle) to (lowermiddle);
\draw[arr,<->] (lowermiddle) to (lowerMiddleDec) ;
\draw[arr,<->] (middle) to (middleDec);
\draw[arr,draw=black] 
  (upper) -- 
  ++(0pt,1cm) node[above] {predicted class};
\draw[arr,dashed,draw=black] 
  (lowermiddle) to[out=45,in=315] (middle);
\draw[arr,dashed,draw=black] 
  (middle) to[out=45,in=315] (upper);


\node[anchor=north west,align=left] 
  at (upper) {\hspace{0.7in} neural network layer}; 
\node[anchor=north west,align=left] 
  at (middleDec) {\\ \hspace{0.7in} decoding layer}; 
\node[anchor=north west,align=left] 
  at (middle) {\hspace{0.7in} encoding layer}; 
\node[anchor=north west,align=left] 
  at (lowerMiddleDec) {\\ \hspace{0.7in} decoding layer}; 
\node[anchor=north west,align=left] 
  at (lowermiddle) {\\ \hspace{0.7in} encoding layer}; 
\end{tikzpicture}
\caption{Deep architecture during training. The training begins with the first
two layers, encoder - decoder, and once the training is complete feature are
encoded and propagated to the third layer in order to further
encode - decode high-level features. Finally, the last neural layer retrieves
the encoded features from the third layer and learns the target class. The
training is performed using SCG descent \cite{le2013building}.}
\label{fig:TrainingArchitecture}
\end{figure}
The first two layers are a set of unsupervised autoencoders that minimize the
loss function in (\ref{eq:loss}). The first layer, i.e., an encoding layer,
receives as input $N$ images of $256\times 256$ as row vectors, each denoted as 
$\mathbf{x}_n \in \mathbb{R}^{65536}$, where $n \in \{1,2, \dots, N\}$. The
training phase encodes the attibutes using 100 neural units to produce
$\mathbf{\hat{x}}_n \in \mathbb{R}^{100}$, and decodes back to
the feature space using, intuitively, 65536 neural units; all neural units use
logistic activation functions. 

Similarly, the third and fourth layers are an encoder and decoder,
respectively. The encoder in the third layer receives as input an encoded
version of the input coming from the first layer, denoted as
$\mathbf{\hat{x}}_n$, and encodes using 50 neural units producing a modified
version of the feature vector denoted as 
$\mathbf{\tilde{x}}_i \in \mathbb{R}^{50}$. 
The decoder in the fourth layer decodes using 100 neural units. 

In the last layer of the model we use a network of 31 neural units with softmax
activation functions. Each neuron is estimulated $\mathbf{\tilde{x}}_n$ and is
trained to predict the probability of the $n$-th sample belonging to a specific
class $C \in \{1, 2, \dots, 31\}$. The output of this layer for the $n$-th
sample is the estimated probability of that sample belonging to all classes, 
formally denoted as $\mathbf{\hat{d}}_n \in \mathbb{R}^{31}$. The layer is trained to minimize
the cross entropy function \cite{szegedy2016rethinking} given by:
\begin{align}
E=\frac{1}{N} \sum_{n=1}^N \sum_{c \in C} \hat{d}_{cn} \ln d_{cn} + (1 -
\hat{d}_{cn}) \ln (1 - d_{cn}) 
\label{eq:entropy}
\end{align}
where $\mathbf{d}_n \in \mathbb{R}^{31}$ is the true probability of the $n$-th
sample belonging to a specific class. 

Once the process of training the autoencoders and the softmax layer, the
network undergoes a last refined training phase. In this last process, only the
first, third, and fifth layers are fully connected and trained simmulating a
feed-forward neural network, as shown in Figure \ref{fig:finalArchitecture}. 
\begin{figure}
\centering
\begin{tikzpicture}[node distance=1.5cm and 1.25cm]
\node[redondo=2cm,label={120:\emph{softmax} activations}]
  (upper)
  {};
\pic at (upper.west) {dosp};
\pic at ([xshift=-0.75cm]upper.east) {unop};
\node[] 
  at ([xshift=4pt,yshift=-1pt]$ (upper.center) $ ) {$\ldots$};

\node[redondo=3.3cm,below=of upper,label={140:\emph{logistic} activations}]
  (middle)
  {};
\pic at (middle.west) {tresp};
\pic at ([xshift=-1.04cm]middle) {dosp};
\pic at ([xshift=-0.75cm]middle.east) {dosp};
\node[] at ([xshift=4pt,yshift=-1pt]middle) {$\ldots$};

\node[redondo=6.4cm,below=of
middle,label={140:\emph{logistic} activations}]
  (lowermiddle)
  {};
\pic at (lowermiddle.west) {tresp};
\pic at ([xshift=-1.85cm]lowermiddle) {tresp};
\pic at ([xshift=-2.58cm]lowermiddle) {tresp};
\pic at ([xshift=-1cm]lowermiddle.east) {tresp};
\node[cuadra,below=of lowermiddle,label={right:{input image}}] (cuadramiddle)
{\includegraphics[width=.075\textwidth]{img/S01num3.eps}};

\node[] at ([xshift=4pt,yshift=-1pt]lowermiddle) {$\ldots$};

\draw[arr] (cuadramiddle) -- ++(0pt,1.5cm) node[above] {$\mathbf{x}_{n} \in \mathbb{N}_+^{65536}$};
\draw[arr] (lowermiddle) -- ++(0pt,1cm) node[above] {$\mathbf{\hat{x}}_n \in \mathbb{R}_+^{100}$};
\draw[arr] (middle) -- ++(0pt,1cm) node[above] {$\mathbf{\tilde{x}}_n \in \mathbb{R}_+^{50}$};
\draw[arr] 
  (upper) -- 
  ++(0pt,1cm) node[above] {output $\mathbf{d}_n \in \mathbb{R}_+^{31}$};
\node[anchor=north west,align=left] 
  at (upper) {\hspace{0.7in} neural network layer}; 
\node[anchor=north west,align=left] 
  at (middle) {\hspace{0.7in} sparse encoding layer}; 
\node[anchor=north west,align=left] 
  at (lowermiddle) {\\ \hspace{0.7in} sparse encoding layer}; 
\end{tikzpicture}
\caption{The working architecture when testing the system. The input is any
test image which is passed through the first sparse encoder in the stack,
reducing the dimensionality of the problem to 100. In the second sparse encoder
the feature space is further reduced to 50. The output are probabilistic
outputs corresponding to each of the 31 classes.}
\label{fig:finalArchitecture}
\end{figure}
The initial weights are those obtained during the
encoding-decoding learning phase and fine tuned using SCG descent to minimize
the cross entropy in (\ref{eq:entropy}).


\section{Experiments}
\subsection{Setup}
The experimental setup was as follows. The dataset was divided in 50\%, 25\%,
and 25\% for training, testing, and validation respectively. Two autoencoders
were trained using SCG to minimize the cross entropy previously defined. The
first autoencoder has 100 neurons and the second one has 50. The second
autoencoder is trained with the reduced features from the first autoencoders. 
A softmax neural network is then trained to minimize the mean squared error
with SCG. 

The process is done once per each subject to match the experiments done by Kang
et. al. in \cite{kang2015real}. The data was not processed in any other manner
before being presented to the neural architecture for training nor testing.
Thus, the inputs are row-vector versions of 8-bit depth images as shown in
Figure \ref{fig:sampN1}.


\subsection{Results and Discussion}
Performance results on the validation sets are shown in Table
\ref{tbl:performance}. 
\begin{table}
\centering
\caption{Performance of the deep architecture over the validation sets.}
\begin{tabular}{ccccccc}
\hline\noalign{\smallskip}
 & S1 & S2 & S3 & S4 & S5 & Avg. \\ \noalign{\smallskip}
\hline
\noalign{\smallskip}
ACC & 0.9748 & 0.9923 & 0.9935 & 0.9929 & 0.9910 & 0.9889 \\ 
SPC & 0.9991 & 0.9997 & 0.9998 & 0.9998 & 0.9997 & 0.9996 \\
MAE & 0.1483 & 0.0640 & 0.0373 & 0.0347 & 0.0494 & 0.0667\\ \hline
\end{tabular}
\label{tbl:performance}
\end{table}
The table indicates that the average accuracy was of 98.9\% which is similar
to the results reported in \cite{kang2015real} of 99.9\%. Although the
difference is 1\%, it is not
statistically signifficant. Furthermore, the deep architecture proposed in this
paper is more efficient than the CNN proposed by the authors. The accuracy
(ACC) is calculated as $\frac{TP+TN}{TP+FP+TN+FN}$ where $TP$ is the count of
true positives, $TN$ is the count of true negatives, $FP$ is the count of false
positives, and $FN$ is the count of false negatives.
When it comes to specificity (SPC), defined as $\frac{TN}{FP+TN}$, the proposed
model gives a measure of $0.99$, indicating that the model is very good at
predicting true negatives. This quality is desired in this type of problems to
make sure the model is sure what a sign is not. This property could be
attributed to the minimization of the Kullback-Leibler divergence function
defined in (\ref{eq:divergence}) that causes the average outputs of the neural
unitis to be low. Similarly, a low mean absolute error (MAE), defined as
$\frac{1}{N} \sum_{i=1}^N |d_i - \hat{d}_i|$, is an indicator of low average
error and low uncertainty in the output of the nerual units. The average MAE
was of $0.06$ and it was as slow as $0.03$ for some subjects.

Figure \ref{fig:errTargPred} shows all the errors made after training over the
validation set.
\begin{figure}
\centering
\begin{tikzpicture}
  \begin{axis}[
    width=12.2cm,
    height=10.0cm,
    axis y line*=none,
    axis x line*=bottom,
    xbar stacked,
    bar width=10pt,
    xmin=0,
    ytick=data,
    yticklabels from table={bardata.dat}{y},
    xlabel={Number of total errors $|$ Bars: incorrectly predicted class},
    ylabel={Target Class},
    font=\footnotesize,
    nodes near coords,
    nodes near coords style={
        text=white,
    },
    point meta=explicit symbolic,
  ]
  \foreach \i in {1,...,7} {
    \pgfmathtruncatemacro{\MetaColNo}{7+\i}
    \addplot [
        fill=gray!50!black,
        draw=white,
        font=\scriptsize,
    ] table [
      x index=\i,
      y expr=\coordindex,
      meta index=\MetaColNo,
    ] {bardata.dat};
  }
  \end{axis}
\end{tikzpicture}
\caption{Stacked bar chart of classification errors. Every stacked bar
indicates the predicted class and the size indicates frequency amount. Common
errors are with hand signs that involve a closed hand gesture.}
\label{fig:errTargPred}
\end{figure}
The firgure indicates that the most common error is with the sign for the
letter `E' which is confused with the letters `S', `M', and `T'. A close
inspection of Figure \ref{fig:asl} reveals that these signs are very similar
and these errors are not surprising. Similarly, the sign `M' was missclassified
as `A' and `S'. Most misclassifications in Figure \ref{fig:errTargPred} are
related to signs done when the hand is making a fist form and the differences
in depth are low. 

To illustrate this further, Figure \ref{fig:errors} shows a variety of
missclassification errors. 
\begin{figure}
\centering
\begin{tabular}{cccc}
Input & Predicted$^*$ & Input & Predicted$^*$ \\

\includegraphics[width=1.5cm]{img/S01_C14_0186.eps} & 
\includegraphics[width=1.5cm]{img/S01_C28_0015.eps} &  
\includegraphics[width=1.5cm]{img/S01_C22_0139.eps} & 
\includegraphics[width=1.5cm]{img/S01_C28_0142.eps} \\ 
$\text{E}$ & $\text{T}$ & $\text{N}$ & $\text{T}$ \\

\includegraphics[width=1.5cm]{img/S02_C28_0199.eps} & 
\includegraphics[width=1.5cm]{img/S02_C10_0146.eps} & 
\includegraphics[width=1.5cm]{img/S03_C10_0138.eps} & 
\includegraphics[width=1.5cm]{img/S03_C24_0055.eps} \\
$\text{T}$ & $\text{A}$ & $\text{A}$ & $\text{P}$ \\

\includegraphics[width=1.5cm]{img/S01_C04_0118.eps} & 
\includegraphics[width=1.5cm]{img/S01_C06_0014.eps} & 
\includegraphics[width=1.5cm]{img/S01_C21_0058.eps} & 
\includegraphics[width=1.5cm]{img/S01_C22_0123.eps} \\
$\text{4}$ & $\text{6/W}$ & $\text{M}$ & $\text{N}$ \\

\includegraphics[width=1.5cm]{img/S04_C05_0138.eps} & 
\includegraphics[width=1.5cm]{img/S04_C06_0073.eps} & 
\includegraphics[width=1.5cm]{img/S01_C06_0015.eps} & 
\includegraphics[width=1.5cm]{img/S01_C07_0013.eps} \\
$\text{5}$ & $\text{6/W}$ & $\text{6/W}$ & $\text{7}$ \\

\end{tabular}
\caption{Examples of classification errors. $^*$The image shown as predicted is
the nearest neighbor found in the predicted class with respect to the input.}
\label{fig:errors}
\end{figure}
From the figure it can be seen that hand gestures
with a closed fist can be consued with other similar gestures. However, a close
inspection of other gestures suggests that errors are also caused by poor
segmentation of the hand area. For example, consider the signs corresponding to
numbers `4' and `6', and also letters `E', `A', and `M', the lower portion of
the image has a section that was not segmented out of the image and it has been
introduced as part of the input. It is possible that the network encoded this
as a distinctive feature of the images leading to classification errors. 
Figure \ref{fig:weights} visualizes the weights associated to each neural
unit in the first encoding layer for the fifth subject. 
\begin{figure}
\centering
\includegraphics[width=12.2cm]{img/weightsS05.eps}
\caption{Weights associated with 100 trained neurons in the first layer for the
fifth subject.}
\label{fig:weights}
\end{figure}
The weights are capturing higer-level
features and at the same time some seem to discard inforation around the limits
of the images, but some seem to rely on regions of the image that are
consistent with regions where images exhibit poor segmentation. Further studies
are required to confirm this, however.

As shown in Table \ref{tbl:performance}, the overall accuracy is 98.9\%, and
furthermore, Figure \ref{fig:errPerClass} shows the average accuracy for each
individual sign. 
\begin{figure}
\centering
\begin{tikzpicture}
  \begin{axis} [   
    width=12.2cm,
    height=7.0cm,
    ybar,
    xlabel=Sign,
    ylabel=Accuracy,
    xtick=data,
    xticklabels from table={classbardata.dat}{Sign},
    bar width=10pt,
    x tick label style={font=\footnotesize,align=right,rotate=90},
    enlarge x limits=0.01,
    enlarge y limits=0.01,
    ]
    \addplot [
      fill=gray!50!black,
      draw=white,
      ] table [
      x=Position,
      y=Accuracy
    ] 
    {classbardata.dat};
  \end{axis}
\end{tikzpicture}
\caption{Average accuracy for each different sign on validation set. Most signs 
with lower accuracy are ones whose sign involves a closed hand gesture.}
\label{fig:errPerClass}
\end{figure}
From the figure it can be seen that signs that involve hand gestures are likely
to have lower accuracy. The figure also shows that about half of the signs
achieve perfect classification over samples that were unseen by the network
architecture. This indicates superior generalization abilities of the proposed
deep architecture. 


\section{Conclusions}

We have presented a deep learning architecture based on sparse autoencoders
for the problem of sign recognition. The model recognizes 31 different signs in
the American Sign Language with an average accuracy of 98.9\%. We proposed an
architecture with 100 and 50 neural units trained in unsuppervised
encoding-decoding steps, and a softmax network with 31 units in the output
layer corresponding to each class. The input consists of depth-sensor images
from five different subjects whose hand sign was segmented to preserve only the
regions corresponding to the hand. Further inspection of classification errors
indicates that errors are most common among signs that are similar and are amog
the type that has a closed hand sign; other causes or missclassification come
from poor segmentation in the original dataset. 

The proposed five layer architecture captures well, in three levels of
abstraction, the intricacies of hand gestures. However, the problem of finding 
a minimal number of neurons withouth loss of
generalization in each layer requires further research. The next steps,
however, are to explore subject vs subject experiments, such as training with a
varying number of subjects and testing with new unseen subjects whose hand
size, shape, and musculoskeletal systems are completely unknown. 



\bibliographystyle{splncs03} 
\bibliography{ref}
\end{document}
